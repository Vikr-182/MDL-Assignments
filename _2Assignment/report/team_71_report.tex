\documentclass[11pt]{article}
\usepackage{amsfonts}%For Maths fonts.
\usepackage{amssymb}%For math symbols.
\usepackage{amsmath}%For the math.
\usepackage{graphicx}%For the image.
\graphicspath{ ./} 
\usepackage{tikz}
\usepackage{float}
\usepackage{hyperref} 
\usepackage{listings}
\usepackage{color}
\usepackage{csvsimple}
\usepackage{dirtree}
\usepackage{graphicx}
\usepackage{amsbsy}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\usetikzlibrary{shapes.geometric, arrows}%For the borders and shapes.
\begin{document}
	\begin{titlepage}
		\begin{flushleft}
			
			\bf CS7.301
			\hfill
			\bfseries 13$^{th}$ April, 2020
		\end{flushleft}
		\begin{center}
			\line(1,0){300}\\
			[5mm]
			\huge{\bfseries MDL Assignment-2 Part-3}\\
			\line(1,0){300}\\
			[12cm]
		\end{center}
		\begin{flushright}
			{
				\line(1,0){270}\\
				\large  Vikrant Dewangan, Roll No.- 2018111024\\  Mohammad Nomaan Qureshi,
				Roll No.- 2018111027
				\line(1,0){270}\\	
			}
		\end{flushright}
	\end{titlepage}
	\newpage
	\tableofcontents
	\newpage
	\section{File Structure}
	Upon running the file \textit{solution.py}, we get 
	\dirtree{%
		.1 team\.71.
		.2 team\_71\_report.pdf: contains report.
		.2 output: .
		.3 output.json: Containing the output.
		.2 solution.py: main solution.
	}

	\section{Problem Statement}
	The same as in Assignment 2, only this time, there is no reward for Lero to reach the terminal state. \\
	\textbf{Given:} Initially, Lero has 3 arrows, full stamina while MD also has full health. 
	Thus, 
	\begin{align*}
		\alpha_{[4,3,2]} &= 1.0.		
	\end{align*}
	\textbf{Assumption} - Order of actions to be \{NOOP, SHOOT, DODGE, RECHARGE\} .
	\section{L.P Formulation}
	The linear programming formulation to MDP is given as - 
	\begin{align*}
		\text{Maximize} & \sum V_i
	\end{align*}
	such that 
	\begin{align*}
		V_i & \leq [R(I,A) + \gamma \sum P(J|I,A) \cdot V_j]
	\end{align*}
	where $V_i$ is the value of the $i_{th}$ state.

Now as per our problem, we state the formulation as follows - 
	\begin{align*}
		\text{Max } & (\textbf{r} \cdot \textbf{x})
	\end{align*}
	such that 
	\begin{align*}
		\textbf{A} \cdot \textbf{x} &= \boldsymbol{\alpha} \\
		x_i &\geq 0 \text{ ,  } \forall \text{}  x_i \in \textbf{x}
	\end{align*}
	where 
	\begin{align*}
		\text{\textbf{r}} & : \text{stands for list of rewards for each (state, action).}\\		
		\text{\textbf{x}} & : \text{expected number of times of each action in (state, action).}\\		
		\text{\textbf{A}} & : \text{Transition probability matrix.}\\		
		\boldsymbol{\alpha} & : \text{Initial probability distribution.}	
	\end{align*}
	\subsection{Preparing the A matrix}
	Before making the \textbf{A} matrix, we first need to know the size of our \textbf{r} matrix.
	The following cases arrive - 
	\begin{enumerate}
		\item When enemy health is 0 : Since this is terminal state, the only action performed in this case would be NOOP.
		Total 12 such cases.
		\item When enemy health is not 0 (NOOP action not considered): 
			\begin{enumerate}
				\item When stamina is 0 : Only option there is to RECHARGE. Total 16 such cases.
				\item When stamina is 50: 
				\begin{enumerate}
					\item When arrows are zero : SHOOT can't happen. Hence only option is RECHARGE, DODGE. Total 4 such cases.
					\item When arrows are not zero : All the actions are possible. Total 12 such cases.
				\end{enumerate}
				\item When stamina is 100: 
				\begin{enumerate}
					\item When arrows are zero : SHOOT can't happen. Hence only option is DODGE. Total 4 such cases.
					\item When arrows are not zero : All the actions are possible except for RECHARGE. Total 12 such cases.
				\end{enumerate} 
			\end{enumerate}
	\end{enumerate}
	Thus the length of \textbf{r} array would be 
	\begin{align*}
		12\times1 + 16\times1 + 4\times2 + 12\times3 + 4\times1 + 12\times2 &= 100. 
	\end{align*}
	Now out of which we know, the reward would be 0 only when either it is at a terminal state, or when having non-finite number of arrows, non-zero stamina and 
	enemy in pen-ultimate health level and action chosen is SHOOT. That is,
\[
r_{[i,j,k]} =
\begin{cases}
	0 \text{ when } i = 0\\ 
	-5 \text{ otherwise }
\end{cases}
\]
	The dimensions of A matrix are the 
	\begin{align*}
		& \text{number of states} \times \text{number of variables}  \\
		=\text{  }& 60 \times 100 
	\end{align*}
	Each row would correspond to a state and each column would correspond to a variable that is, an action taken in a particular state. 
	For each element of the matrix $A_{ij}$, it denotes the transition probability of the action $j$ \textbf{FROM} state $i$. It is negative
	if the action leads Lero into that state and positive if it is originated from it.
	Using this, we can construct our matrix A - 
	\begin{center}
		A =
	$ 
	\begin{pmatrix}
		1 & 0 & 0 &\ldots &0&0&0\\
		0 & 1 & 0 &\ldots &0&0&0\\
		0 & 0 & 1 &\ldots &0&0&0\\
		\vdots & \vdots & \vdots & \ldots & \vdots \\
		0 & 0 & 0 &\ldots &0&0&-0.04\\
		0 & 0 & 0 &\ldots &0.8&0&-0.16\\
		0 & 0 & 0 &\ldots &-0.8&1&1
	\end{pmatrix}		
		$
	\end{center}
		Thus the A matrix is obtained.
		\subsection{Rendering the policy}
		Following procedure is followed - 
		\begin{itemize}
	\item 		Upon solving the linear programming and obtaining \textbf{x} , we get a linear array of values each being \textbf{deterministic} uniformly
	optimal policy. We get it as follows - 
	\begin{align*}
		\textbf{x} &= [x_{1a1},x_{1a2},x_{2a1},x_{3a1},x_{3a2} \ldots ]
	\end{align*}
	Either of $x_{nai}$ is non zero and all being non zeroes confirming to a deterministic policy.
\item 			For example, we get for the stage [1,1,2], SHOOT is the best option. We iterate over the \textbf{x} array and check for all actions from a particular 
state and choose the one with the highest probability.

		\end{itemize}
		\subsection{Changing the policies}
		The final policy is dependent on a number of factors, including especially the \textbf{order} of policies taken.
		For example, if we have SHOOT before RECHARGE, then in case of a tie, SHOOT would be given priority before RECHARGE.
		The way it affects is as follows - 
		\begin{itemize}
			\item Changing the order of actions changes the way A matrix is written. Say, if earlier order was NOOP,SHOOT, RECHARGE and then DODGE, if the order 
			is changed, it would lead to different actions within a state reaching to different states. 
			\item If a particular action changes, so does it's reward. In this case the \textbf{r} matrx will have it's columns juggled. This will lead to a change 
			in the solution of linear programming to appropriately max out the variables and the results coefficients \textbf{x} we get would not be the same.
			Thus, what we would see is a newer policy implemented.
			\item Changing the $\boldsymbol{\alpha}$ vector also changes the way we approach the problem. In this problem, we are assuming that $\boldsymbol{\alpha}$ is given to us.
			However, a different value of $\boldsymbol{\alpha}$ may lead to a completely different policy. Say if I am in a terminal state at the start, then I know I will only perform
			NOOP action and that will be my optimal policy. 
		\end{itemize}
\end{document}

